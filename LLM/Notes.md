# Models 

- https://www.vellum.ai/llm-leaderboard 

### Paid Models
- GPT from OpenAI
- Claude from Anthropic
- Gemini from Google
- Command R from Cohere
- Perplexity

<img width="949" alt="image" src="https://github.com/user-attachments/assets/18acac2a-6539-440b-a532-34bc5043ee6c" />

### OpenSource Models
- Llama from Meta
- Mixtral from Mistral
- Qwen from Alibaba Cloud
- Gemma from Googl
- Phi from Microsoft

  <img width="947" alt="image" src="https://github.com/user-attachments/assets/f039fee4-ca32-473f-93ee-5ed16472609d" />

## RAG
- https://github.com/langchain-ai/rag-from-scratch
- https://www.pinecone.io/learn/retrieval-augmented-generation/
  ![image](https://github.com/user-attachments/assets/fff413a1-e702-4bd6-8b60-fa69660a135c)


### Overview of LLM
- Large language models (LLMs) are defined by the number of parameters, also called weights, which control their output.
- Traditional machine learning models typically have between 20 and 200 parameters, whereas LLMs have millions to trillions of parameters.
- LLM Models are trained and operated by tokens - https://platform.openai.com/tokenizer
- https://www.gradio.app/

## Embedding

<img width="805" alt="image" src="https://github.com/user-attachments/assets/2a621c06-aaf8-4ae8-800d-81f5c6596bc3" />

<img width="765" alt="image" src="https://github.com/user-attachments/assets/1567aab2-bbad-4eeb-a9f6-77fa349dfa56" />


- https://simonwillison.net/2023/Oct/23/embeddings/
- https://python.langchain.com/docs/concepts/embedding_models/
- https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity

## MCP, ADK and A2A

- https://codelabs.developers.google.com/codelabs/currency-agent?hl=en#0
